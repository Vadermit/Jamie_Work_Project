{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long-short Term Memory(LSTM)\n",
    "LSTM is actually also a special kind of neural network. The major difference between LSTM and RNN is that LSTM has a special mechanism called **forget gate**. As RNNs do, LSTMs also have **hidden state** that would pass to the next time slot. But not like RNN, iformations are selected through the forget gate and only let through those informations that is usefull. Let see how this would help LSTM get away from gradient vanish or gradient explode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input gate in LSTM\n",
    "First, the input is squashed between -1 and 1 using a tanh activation function. This can be expressed by:\n",
    "\n",
    "$$g = tanh(b^g+x_tU^g+y_{t-1}V^g)$$\n",
    "\n",
    "Where **$U^g$** and **$V^g$** are the weights for the input and previous cell output, respectively, and **$b^g$** is the input bias. Note that the exponents **g** are not a raised power, but rather signify that these are the input weights and bias values (as opposed to the input gate, forget gate, output gate etc.).\n",
    "\n",
    "This squashed input is then multiplied element-wise by the output of the input gate. The input gate is basically a hidden layer of sigmoid activated nodes, with weighted **$x_t$** and **$y_{t-1}$** input values, which outputs values of between 0 and 1 and when multiplied element-wise by the input determines which inputs are switched on and off. In other words, it is a kind of input filter or gate. The expression for the input gate is:\n",
    "\n",
    "$$i = \\sigma(b^i + x_tU^i+ y_{t-1}V^i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The hidden state and the forget gate\n",
    "Forget gate is again a sigmoid activated set of nodes which is element-wise multiplied by the hidden state of the previous moment **$s_{t-1}$** to determine which previous states should be remembered (i.e. forget gate output close to 1) and which should be forgotten (i.e. forget gate output close to 0). This allows the LSTM cell to learn appropriate context. The forget gate is like:\n",
    "\n",
    "$$f = \\sigma(b^f + x_tU^f + y_{t-1}V^i)$$\n",
    "\n",
    "So the hidden state of the current moment is:\n",
    "\n",
    "$$s_t = s_{t-1}\\circ f + g \\circ i$$\n",
    "\n",
    "Where $\\circ$ denotes element-wise multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The output gate in LSTM\n",
    "\n",
    "The final stage of the LSTM cell is the output gate. The output gate has two components â€“ another tanh squashing function and an output sigmoid gating function. The output sigmoid gating function, like the other gating functions in the cell, is multiplied by the squashed state st to determine which values of the state are output from the cell. \n",
    "\n",
    "The output gate is like:\n",
    "\n",
    "$$o = \\sigma(b^o + x_tU^o + y_{t-1}V^o)$$\n",
    "\n",
    "So the final output of the cell is:\n",
    "\n",
    "$$y_t = tanh(s_t)\\circ o$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import math\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the corpus\n",
    "\n",
    "Before we start to setup our LSTM network, we first need to create the word vector respresentations(embeddings) of words in the corpus. Those words imbeddings are the inputs to our LSTM network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text processing\n",
    "This part mainly did two things:\n",
    "* Read the corpus file.\n",
    "* Divide the corpus into individual words.\n",
    "\n",
    "The corpus used in this tutorial can be found in my GitHub repositry, or you can download it in mattmahoney.net/dc/text8.zip (not available in China mainland).\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The forst 100 letters in the corpus are:\n",
      " anarchism originated as a term of abuse first used against early working class radicals including t\n"
     ]
    }
   ],
   "source": [
    "#Read the file\n",
    "f = open(\"text8\",\"r\")\n",
    "rawData = f.read()\n",
    "print(\"The forst 100 letters in the corpus are:\")\n",
    "print(rawData[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 words in the corpus\n",
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "# Transfer the raw data as strings using Tensorflow. \n",
    "# Then split it into individual words\n",
    "dataStr = tf.compat.as_str(rawData) #Convert to string\n",
    "print('The first 10 words in the corpus')\n",
    "data = dataStr.split() #Split by blank\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Word2Index & Index2Word dictionary\n",
    "Words need to be represented by something the computer would understand which are numbers. So basically we would use numbers (we call them indeces) to represent those. In this part, the following two tasks are accomplished:\n",
    "* Select 9999 different most frequent words(plus one 'Less frequent words class').\n",
    "* Using indeces to represent each word: build Word2Index dictionary.\n",
    "* Build the inverse dictionary - Index2Word dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253854 different words were found in the corpus\n",
      "\n",
      "Part of the dictionary:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'anarchism': 303,\n",
       " 'originated': 572,\n",
       " 'as': 131815,\n",
       " 'a': 325873,\n",
       " 'term': 7219,\n",
       " 'of': 593677,\n",
       " 'abuse': 563,\n",
       " 'first': 28810,\n",
       " 'used': 22737,\n",
       " 'against': 8432}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counted_words = collections.Counter(data)  #counted_words is a dictionary {'word1': num_1, 'word2': num_2, ...}\n",
    "print('%d different words were found in the corpus'%(len(counted_words)))\n",
    "print()\n",
    "print('Part of the dictionary:')\n",
    "dict(list(counted_words.items())[0:10])    #print 10 of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9999 words in the frequent words dictionary\n"
     ]
    }
   ],
   "source": [
    "freq_counted_words = dict(counted_words.most_common(9999))\n",
    "print('There are %d words in the frequent words dictionary'%(len(freq_counted_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10000 words in the dictionary(plus less frequent words class)\n",
      "\n",
      "The first 5 entries in the dictionary is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lfw': 243855, 'the': 1061396, 'of': 593677, 'and': 416629, 'one': 411764}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfw_count = 0\n",
    "for word in counted_words:\n",
    "    if not (word in freq_counted_words.keys()):\n",
    "        lfw_count += 1\n",
    "word_count_dict = {'lfw': lfw_count}\n",
    "word_count_dict.update(freq_counted_words)\n",
    "print('There are %d words in the dictionary(plus less frequent words class)'%(len(word_count_dict)))\n",
    "print()\n",
    "print('The first 5 entries in the dictionary is:')\n",
    "dict(list(word_count_dict.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 entries in the dictionary \"word_dict\" is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lfw': 0, 'the': 1, 'of': 2, 'and': 3, 'one': 4}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#index words with numbers aka establish the word2number projection\n",
    "ind = 0\n",
    "word_dict = {}\n",
    "for word in word_count_dict:\n",
    "    word_dict.update({word: ind})\n",
    "    ind += 1\n",
    "print('The first 5 entries in the dictionary \"word_dict\" is: ')\n",
    "dict(list(word_dict.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 entries in dictionary index_dict is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'lfw',\n",
       " 1: 'the',\n",
       " 2: 'of',\n",
       " 3: 'and',\n",
       " 4: 'one',\n",
       " 5: 'in',\n",
       " 6: 'a',\n",
       " 7: 'to',\n",
       " 8: 'zero',\n",
       " 9: 'nine'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build reverse dictionary --- Index2Word dictionary\n",
    "index_dict = dict(zip(word_dict.values(), word_dict.keys()))\n",
    "print('The first 10 entries in dictionary index_dict is: ')\n",
    "dict(list(index_dict.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer the text sequence into index sequence\n",
    "\n",
    "Now we have the Word2Index dictionary, we can rerepresent the corpus using index sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 words in the corpus represented by their corresponding indeces are:\n",
      "[5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156]\n"
     ]
    }
   ],
   "source": [
    "num_corpus = []\n",
    "for word in data:\n",
    "    if word in word_dict.keys():\n",
    "        num_corpus.append(word_dict[word])\n",
    "    else:\n",
    "        num_corpus.append(word_dict['lfw'])\n",
    "print('The first 10 words in the corpus represented by their corresponding indeces are:')\n",
    "print(num_corpus[:10])\n",
    "\n",
    "train_data = num_corpus[:80000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we restore the corpus from that number representation to see if they're identiclel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 words restored from the index representation are:\n",
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "restored_cor = []\n",
    "for i in range(10):\n",
    "    restored_cor.append(index_dict[num_corpus[i]])\n",
    "print('The first 10 words restored from the index representation are:')\n",
    "print(restored_cor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Word2Vec neural network with Tensorfow\n",
    "\n",
    "We use a three layers autoencoder to calculate the embedding(vector representation) for each word. Detials of this neural network can be found in my GitHub notebook - **Softmax Word2Vec** tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "# generate batch data\n",
    "def generate_batch(data, batch_size, skip, sub_gram):\n",
    "    global data_index\n",
    "    assert batch_size % skip == 0\n",
    "    assert skip <= 2 * sub_gram\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    context = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * sub_gram + 1  # [ sub_gram input_word sub_gram]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // skip):\n",
    "        target = sub_gram  # input word at the center of the buffer\n",
    "        targets_to_avoid = [sub_gram]\n",
    "        for j in range(skip):\n",
    "            while target in targets_to_avoid:\n",
    "                target = np.random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * skip + j] = buffer[sub_gram]  # this is the input word\n",
    "            context[i * skip + j, 0] = buffer[target]  # these are the context words\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1017 20:33:16.672229  1888 deprecation.py:323] From <ipython-input-12-169572810c5f>:30: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 10000\n",
    "batch_size = 650\n",
    "embedding_size = 650  # Dimension of the embedding vector aka the number of units in the hidden layer.\n",
    "sub_gram = 2          # (Span_of_gram-1)/2\n",
    "skip = 2              # How many times to reuse an input to generate a context.\n",
    "\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "train_context = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "# weight matrix between input layer and hidden layer\n",
    "embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "\n",
    "# get the corresponding embedding of input word.\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "# weight matrix between hidden layer and the softmax layer\n",
    "weights = tf.Variable(tf.truncated_normal([embedding_size, vocabulary_size],\n",
    "                          stddev=1.0 / math.sqrt(embedding_size)))\n",
    "\n",
    "# biases of softmax layer\n",
    "biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "\n",
    "hidden_out = tf.matmul(embed, weights) + biases\n",
    "\n",
    "train_one_hot = tf.one_hot(train_context, vocabulary_size)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hidden_out, \n",
    "    labels=train_one_hot))\n",
    "\n",
    "# Construct the SGD optimizer using a learning rate of 1.0.\n",
    "optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(cross_entropy)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  2000 :  6.614294956207275\n",
      "Average loss at step  4000 :  6.04705004632473\n",
      "Average loss at step  6000 :  6.078072331190109\n",
      "Average loss at step  8000 :  6.123249652862548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1017 21:06:52.049563  1888 deprecation.py:506] From <ipython-input-13-ab29e2220d4f>:30: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  10000 :  6.086918895125389\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-ab29e2220d4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mnormalized_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membeddings\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mfinal_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalized_embeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\\\'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmodel_save_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The size of the embedding matrix is:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sess' is not defined"
     ]
    }
   ],
   "source": [
    "num_steps = 10000\n",
    "model_path = 'models'\n",
    "model_save_name = 'embedding_model'\n",
    "\n",
    "with tf.Session() as session:\n",
    "    saver = tf.train.Saver()\n",
    "    # We must initialize all variables before we use them.\n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_inputs, batch_context = generate_batch(num_corpus,\n",
    "            batch_size, skip, sub_gram)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_context: batch_context}\n",
    "\n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        _, loss_val = session.run([optimizer, cross_entropy], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if (step + 1) % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step ', (step + 1), ': ', average_loss)\n",
    "            average_loss = 0\n",
    "    # Normalize embeddings: scale them by dividing their norm\n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "    saver.save(session, model_path + '\\\\' + model_save_name, global_step=epoch)\n",
    "\n",
    "print('The size of the embedding matrix is:')\n",
    "print(final_embeddings.shape)\n",
    "print('The embedding of word \"the\" is:')\n",
    "print(final_embeddings[1][:100])\n",
    "print('...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the embedding matrix is:\n",
      "(10000, 650)\n",
      "The embedding of word \"the\" is:\n",
      "[-2.6655966e-02 -6.6376761e-02 -1.9247079e-02 -1.2780695e-02\n",
      "  1.2078389e-02  7.5808614e-02  4.4842158e-02  2.5679041e-02\n",
      " -5.2528664e-02  2.2594307e-02  1.8927136e-02  1.7888596e-02\n",
      "  7.0898436e-02 -6.7119035e-03  3.8841940e-02 -2.4650207e-02\n",
      "  2.5805410e-03  3.4857657e-02  3.9091509e-02 -1.5377350e-03\n",
      "  7.8662165e-02  1.9272333e-05 -4.3310743e-02  1.0005997e-02\n",
      "  6.5957971e-02 -1.6092813e-02  3.5364520e-02  4.2544510e-02\n",
      "  2.1110817e-03 -1.9665316e-02  2.4460493e-02 -1.4648095e-02\n",
      " -2.0771667e-02  4.1408874e-02 -5.8948997e-02 -4.7584381e-02\n",
      "  4.8056729e-02 -1.5734762e-02  4.3267258e-02 -5.9940714e-02\n",
      " -2.5871905e-02  1.7673658e-02  5.0935992e-03 -7.4655600e-02\n",
      " -3.6489550e-02 -1.5665328e-02 -3.4987353e-02  3.7250951e-02\n",
      " -5.3153817e-02 -1.4909114e-02  6.0377330e-02  2.2778971e-02\n",
      "  4.4647507e-02 -3.8469475e-02 -7.0016189e-03 -1.7846381e-02\n",
      " -5.8639278e-03  2.3296148e-02 -6.1928079e-02  1.3785200e-02\n",
      "  1.4123711e-02  2.1587877e-02 -6.4047627e-02  6.0665693e-02\n",
      " -5.2902795e-02  4.6796031e-02 -2.9615337e-02 -7.8657568e-02\n",
      "  5.6015987e-02 -5.3161290e-02 -2.2493050e-02 -3.6294077e-02\n",
      " -1.3733819e-02  5.4812793e-02 -1.8959582e-02 -3.0153427e-02\n",
      "  9.8493695e-03  3.4076259e-02 -3.5683736e-02 -5.7157134e-03\n",
      "  3.4460444e-02  3.7273120e-02 -4.3293353e-02  5.6119985e-03\n",
      "  6.2780820e-02 -6.8505287e-02 -3.3389803e-02  4.9722552e-02\n",
      "  1.8955257e-03  3.9903294e-02 -2.2728341e-02 -2.5272658e-02\n",
      "  5.3622287e-02  4.4541936e-02  3.5640150e-02  2.6529595e-02\n",
      "  9.9685900e-03 -6.8072942e-03  5.4987364e-02  4.1660678e-02]\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "print('The size of the embedding matrix is:')\n",
    "print(final_embeddings.shape)\n",
    "print('The embedding of word \"the\" is:')\n",
    "print(final_embeddings[1][:100])\n",
    "print('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create batches for LSTM\n",
    "In this experiment, we are going to perform one word prediction using **MIMO** (multiple in multiple out) LSTM. For example, given a subsentence with **num_steps = 6** like: **\"anarchism originated as a term of\"** , we want the LSTM to predict the subsentence: **\"originated as a term of abuse\"**.\n",
    "\n",
    "We need to create batches with inputs and their corresponding labels. Both inputs and labels are subsentences with same length. Labels delayed one step compared to their inputs as is shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch example\n",
    "For a fake corpus with 18 words(**data_len = 18**), its numerical representation is [0, 1, 2, ..., 15, 16, 17]. Let's say we want to creat batches with **batch_size = 3**, and **num_steps = 2**. \n",
    "* **batch_size** is the number of inputs-labels pairs which are fed into LSTM to calculate a loss.\n",
    "* **num_step** is the length of subsentences the **MIMO** LSTM processing at a time which is demonstrated above.\n",
    "\n",
    "To create batches from the corpus data, we first reshape the corpus into a **[batch_size, batch_len]** sized matrix. Where **batch_len = data_len // batch_size = 18 // 3 = 6**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "\n",
      "[[ 0  1  2  3  4  5]\n",
      " [ 6  7  8  9 10 11]\n",
      " [12 13 14 15 16 17]]\n"
     ]
    }
   ],
   "source": [
    "a = [i for i in range(18)]\n",
    "print(a)\n",
    "b = tf.reshape(a, [3, 6])\n",
    "print()\n",
    "with tf.Session() as sess:  print(b.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the corpus data was reshaped to matrix with size **[batch_size, batch_len]**, we can easily generate batches by segment the matrix. For exaple, we want to generate a batch with **batch_size = 3**, and sunsentence length **num_step = 2**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "[[ 0  1]\n",
      " [ 6  7]\n",
      " [12 13]]\n",
      "\n",
      "Its corresponding label:\n",
      "[[ 1  2]\n",
      " [ 7  8]\n",
      " [13 14]]\n"
     ]
    }
   ],
   "source": [
    "print('Batch 1:')\n",
    "with tf.Session() as sess:  print(b[:, 0:2].eval())\n",
    "print()\n",
    "print('Its corresponding label:')\n",
    "with tf.Session() as sess:  print(b[:, 1:3].eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2:\n",
      "[[ 1  2]\n",
      " [ 7  8]\n",
      " [13 14]]\n",
      "\n",
      "Its corresponding label:\n",
      "[[ 2  3]\n",
      " [ 8  9]\n",
      " [14 15]]\n"
     ]
    }
   ],
   "source": [
    "print('Batch 2:')\n",
    "with tf.Session() as sess:  print(b[:, 1:3].eval())\n",
    "print()\n",
    "print('Its corresponding label:')\n",
    "with tf.Session() as sess:  print(b[:, 2:4].eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3:\n",
      "[[ 2  3]\n",
      " [ 8  9]\n",
      " [14 15]]\n",
      "\n",
      "Its corresponding label:\n",
      "[[ 3  4]\n",
      " [ 9 10]\n",
      " [15 16]]\n"
     ]
    }
   ],
   "source": [
    "print('Batch 3:')\n",
    "with tf.Session() as sess:  print(b[:, 2:4].eval())\n",
    "print()\n",
    "print('Its corresponding label:')\n",
    "with tf.Session() as sess:  print(b[:, 3:5].eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4:\n",
      "[[ 3  4]\n",
      " [ 9 10]\n",
      " [15 16]]\n",
      "\n",
      "Its corresponding label:\n",
      "[[ 4  5]\n",
      " [10 11]\n",
      " [16 17]]\n"
     ]
    }
   ],
   "source": [
    "print('Batch 4:')\n",
    "with tf.Session() as sess:  print(b[:, 3:5].eval())\n",
    "print()\n",
    "print('Its corresponding label:')\n",
    "with tf.Session() as sess:  print(b[:, 4:6].eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, using the reshaped matrix, 4 batches in an epoch can be easily generated. The following function does the exactly the same thing to generated a batch, where **x** and **y** are the aforementioned input and label respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_producer(raw_data, batch_size, num_steps):\n",
    "    raw_data = tf.convert_to_tensor(raw_data, name=\"raw_data\", dtype=tf.int32)\n",
    "\n",
    "    data_len = tf.size(raw_data)\n",
    "    batch_len = data_len // batch_size\n",
    "    data = tf.reshape(raw_data[0: batch_size * batch_len],\n",
    "                      [batch_size, batch_len])\n",
    "\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "\n",
    "    i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n",
    "    x = data[:, i * num_steps:(i + 1) * num_steps]\n",
    "    x.set_shape([batch_size, num_steps])\n",
    "    y = data[:, i * num_steps + 1: (i + 1) * num_steps + 1]\n",
    "    y.set_shape([batch_size, num_steps])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing word prediction using LSTM\n",
    "\n",
    "In the following section, we will create a two layer LSTM followed by a decoding level and a softmax level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input(object):\n",
    "    def __init__(self, batch_size, num_steps, data):\n",
    "        self.batch_size = batch_size  #number of samples in a batch\n",
    "        self.num_steps = num_steps    #length of sample subsentences\n",
    "        self.epoch_size = ((len(data) // batch_size) - 1) // num_steps  #number of batches in an epoch\n",
    "        self.input_data, self.targets = batch_producer(data, batch_size, num_steps)  #generate a batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the main model\n",
    "class Model(object):\n",
    "    def __init__(self, input, is_training, hidden_size, vocab_size, num_layers,\n",
    "                 dropout=0.5, init_scale=0.05):\n",
    "        self.is_training = is_training        # a index of weather back propagate\n",
    "        self.input_obj = input\n",
    "        self.batch_size = input.batch_size\n",
    "        self.num_steps = input.num_steps        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add dropout to the input\n",
    "The **dropout** reduce the chance of overfitting by adding noise to the inputs. It does this by discarding some elements in the input with certain rate $dropout$. \n",
    "\n",
    "* For elements which are discarded, **dropout** makes theirs values equal 0.\n",
    "* For elements which are reserved, **dropout** scales their initial value by $\\frac{1}{1-dropout}$.\n",
    "\n",
    "The summation of the inputs remains unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-dd501411189b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_training\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdropout\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "if self.is_training and dropout < 1:\n",
    "    inputs = tf.nn.dropout(inputs, dropout)        #add drop out to inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set previous output/inner state\n",
    "Create placeholder for previous output **$h_{t-1}$** and inner state **$s_{t-1}$**. The **init_state** placeholder has the shape of **[num_layers, 2, batch_size, hidden_size]**.\n",
    "* **num_layers** is the stack number(layer number) of stacked LSTM(one LSTM layer stacked over another LSTM layer). \n",
    "* **2** represents there are two other inputs besides the current word embedding. The two other inputs are previous output and previous inner state **$s_{t-1}$**.\n",
    "* **batch_size** is the batch size which is the number of samples passed in the neural network to calculate a loss.(Different samples in a batch have different previous outputs/ inner states.)\n",
    "* **hidden_size** is the size of output **$h$**, inner state **$s$**. It is also the size of embedding of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_layers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-fcb486bbfd80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# set up the state storage / extraction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'num_layers' is not defined"
     ]
    }
   ],
   "source": [
    "# set up the state storage / extraction\n",
    "self.init_state = tf.placeholder(tf.float32, [num_layers, 2, self.batch_size, self.hidden_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the state into structure suits for Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-699afcf5dc49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstate_per_layer_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m rnn_tuple_state = tuple(\n\u001b[0;32m      3\u001b[0m             [tf.contrib.rnn.LSTMStateTuple(state_per_layer_list[idx][0], state_per_layer_list[idx][1])\n\u001b[0;32m      4\u001b[0m              for idx in range(num_layers)]\n\u001b[0;32m      5\u001b[0m         )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "state_per_layer_list = tf.unstack(self.init_state, axis=0)\n",
    "rnn_tuple_state = tuple(\n",
    "            [tf.contrib.rnn.LSTMStateTuple(state_per_layer_list[idx][0], state_per_layer_list[idx][1])\n",
    "             for idx in range(num_layers)]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LSTM cell followed by dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an LSTM cell to be unrolled\n",
    "cell = tf.contrib.rnn.LSTMCell(hidden_size, forget_bias=1.0)\n",
    "# add a dropout wrapper if training\n",
    "if is_training and dropout < 1:\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack LSTM cell to two layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_layers > 1:\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([cell for _ in range(num_layers)], state_is_tuple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unrolling LSTM cell\n",
    "All the **num_step** inputs flows through the same stacked LSTM cells repeatedly. This is done by the following function which feed in inputs of size [batch_size, num_steps, hidden_size] and outputs tensor of the same size. \n",
    "For one sample in a batch, the **dynamic_rnn** function sequentailly feed time steps to the stacked LSTM cell **num_steps** times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, self.state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32, initial_state=rnn_tuple_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatting LSTM outputs and Decoding\n",
    "To perform the decoding process, the LSTM output tensor of size [batch_size, num_step, hidden_size] are transformed to size [batch_size*num_step, hidden_size] first. \n",
    "\n",
    "Then, the word embeddings with size **hidden_state** are transformed to embeddings with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape to (batch_size * num_steps, hidden_size)\n",
    "output = tf.reshape(output, [-1, hidden_size])\n",
    "\n",
    "softmax_w = tf.Variable(tf.random_uniform([hidden_size, vocab_size], -init_scale, init_scale))\n",
    "softmax_b = tf.Variable(tf.random_uniform([vocab_size], -init_scale, init_scale))\n",
    "logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "Raw logits without softmax are directlly used to calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape logits to be a 3-D tensor for sequence loss\n",
    "logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])\n",
    "\n",
    "# Use the contrib sequence loss and average over the batches\n",
    "loss = tf.contrib.seq2seq.sequence_loss(\n",
    "            logits,\n",
    "            self.input_obj.targets,\n",
    "            tf.ones([self.batch_size, self.num_steps], dtype=tf.float32),\n",
    "            average_across_timesteps=False,\n",
    "            average_across_batch=True)\n",
    "# Update the cost\n",
    "self.cost = tf.reduce_sum(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax normalization and accuracy calculation\n",
    "To calculate accuracy, softmax activation is used to normalize logits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the prediction accuracy\n",
    "self.softmax_out = tf.nn.softmax(tf.reshape(logits, [-1, vocab_size]))\n",
    "self.predict = tf.cast(tf.argmax(self.softmax_out, axis=1), tf.int32)\n",
    "correct_prediction = tf.equal(self.predict, tf.reshape(self.input_obj.targets, [-1]))\n",
    "self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_training:\n",
    "   return\n",
    "self.learning_rate = tf.Variable(0.0, trainable=False)\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars), 5)\n",
    "optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "self.train_op = optimizer.apply_gradients(\n",
    "            zip(grads, tvars),\n",
    "            global_step=tf.contrib.framework.get_or_create_global_step())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.new_lr = tf.placeholder(tf.float32, shape=[])\n",
    "self.lr_update = tf.assign(self.learning_rate, self.new_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the main model\n",
    "class Model(object):\n",
    "    def __init__(self, input, is_training, hidden_size, vocab_size, num_layers,\n",
    "                 dropout=0.5, init_scale=0.05):\n",
    "        self.is_training = is_training\n",
    "        self.input_obj = input\n",
    "        self.batch_size = input.batch_size\n",
    "        self.num_steps = input.num_steps\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "#         # create the word embeddings\n",
    "#         with tf.device(\"/cpu:0\"):\n",
    "        embedding = tf.Variable(tf.random_uniform([vocab_size, self.hidden_size], -init_scale, init_scale))\n",
    "#         embedding = tf.Variable(embeddings, trainable = False)\n",
    "        inputs = tf.nn.embedding_lookup(embedding, self.input_obj.input_data)\n",
    "\n",
    "        if is_training and dropout < 1:\n",
    "            inputs = tf.nn.dropout(inputs, dropout)\n",
    "\n",
    "        # set up the state storage / extraction\n",
    "        self.init_state = tf.placeholder(tf.float32, [num_layers, 2, self.batch_size, self.hidden_size])\n",
    "\n",
    "        state_per_layer_list = tf.unstack(self.init_state, axis=0)\n",
    "        rnn_tuple_state = tuple(\n",
    "            [tf.contrib.rnn.LSTMStateTuple(state_per_layer_list[idx][0], state_per_layer_list[idx][1])\n",
    "             for idx in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        # create an LSTM cell to be unrolled\n",
    "        cell = tf.contrib.rnn.LSTMCell(hidden_size, forget_bias=1.0)\n",
    "        # add a dropout wrapper if training\n",
    "        if is_training and dropout < 1:\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=dropout)\n",
    "        if num_layers > 1:\n",
    "            cell = tf.contrib.rnn.MultiRNNCell([cell for _ in range(num_layers)], state_is_tuple=True)\n",
    "\n",
    "        output, self.state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32, initial_state=rnn_tuple_state)\n",
    "        # reshape to (batch_size * num_steps, hidden_size)\n",
    "        output = tf.reshape(output, [-1, hidden_size])\n",
    "\n",
    "        softmax_w = tf.Variable(tf.random_uniform([hidden_size, vocab_size], -init_scale, init_scale))\n",
    "        softmax_b = tf.Variable(tf.random_uniform([vocab_size], -init_scale, init_scale))\n",
    "        logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n",
    "        # Reshape logits to be a 3-D tensor for sequence loss\n",
    "        logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])\n",
    "\n",
    "        # Use the contrib sequence loss and average over the batches\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(\n",
    "            logits,\n",
    "            self.input_obj.targets,\n",
    "            tf.ones([self.batch_size, self.num_steps], dtype=tf.float32),\n",
    "            average_across_timesteps = False,\n",
    "            average_across_batch = True)\n",
    "\n",
    "        # Update the cost\n",
    "        self.cost = tf.reduce_sum(loss)\n",
    "\n",
    "        # get the prediction accuracy\n",
    "        self.softmax_out = tf.nn.softmax(tf.reshape(logits, [-1, vocab_size]))\n",
    "        self.predict = tf.cast(tf.argmax(self.softmax_out, axis=1), tf.int32)\n",
    "        correct_prediction = tf.equal(self.predict, tf.reshape(self.input_obj.targets, [-1]))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        if not is_training:\n",
    "           return\n",
    "        self.learning_rate = tf.Variable(0.0, trainable=False)\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars), 5)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "        # optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train_op = optimizer.apply_gradients(\n",
    "            zip(grads, tvars),\n",
    "            global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "        # self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.cost)\n",
    "\n",
    "        self.new_lr = tf.placeholder(tf.float32, shape=[])\n",
    "        self.lr_update = tf.assign(self.learning_rate, self.new_lr)\n",
    "        \n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(self.lr_update, feed_dict={self.new_lr: lr_value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the main model\n",
    "class Model_pre_embedding(object):\n",
    "    def __init__(self, input, embeddings, is_training, hidden_size, vocab_size, num_layers,\n",
    "                 dropout=0.5, init_scale=0.05):\n",
    "        self.is_training = is_training\n",
    "        self.input_obj = input\n",
    "        self.batch_size = input.batch_size\n",
    "        self.num_steps = input.num_steps\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "#         # create the word embeddings\n",
    "#         with tf.device(\"/cpu:0\"):\n",
    "            #embedding = tf.Variable(tf.random_uniform([vocab_size, self.hidden_size], -init_scale, init_scale))\n",
    "        embedding = tf.Variable(embeddings, trainable = False)\n",
    "        inputs = tf.nn.embedding_lookup(embedding, self.input_obj.input_data)\n",
    "\n",
    "        if is_training and dropout < 1:\n",
    "            inputs = tf.nn.dropout(inputs, dropout)\n",
    "\n",
    "        # set up the state storage / extraction\n",
    "        self.init_state = tf.placeholder(tf.float32, [num_layers, 2, self.batch_size, self.hidden_size])\n",
    "\n",
    "        state_per_layer_list = tf.unstack(self.init_state, axis=0)\n",
    "        rnn_tuple_state = tuple(\n",
    "            [tf.contrib.rnn.LSTMStateTuple(state_per_layer_list[idx][0], state_per_layer_list[idx][1])\n",
    "             for idx in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        # create an LSTM cell to be unrolled\n",
    "        cell = tf.contrib.rnn.LSTMCell(hidden_size, forget_bias=1.0)\n",
    "        # add a dropout wrapper if training\n",
    "        if is_training and dropout < 1:\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=dropout)\n",
    "        if num_layers > 1:\n",
    "            cell = tf.contrib.rnn.MultiRNNCell([cell for _ in range(num_layers)], state_is_tuple=True)\n",
    "\n",
    "        output, self.state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32, initial_state=rnn_tuple_state)\n",
    "        # reshape to (batch_size * num_steps, hidden_size)\n",
    "        output = tf.reshape(output, [-1, hidden_size])\n",
    "\n",
    "        softmax_w = tf.Variable(tf.random_uniform([hidden_size, vocab_size], -init_scale, init_scale))\n",
    "        softmax_b = tf.Variable(tf.random_uniform([vocab_size], -init_scale, init_scale))\n",
    "        logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n",
    "        # Reshape logits to be a 3-D tensor for sequence loss\n",
    "        logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])\n",
    "\n",
    "        # Use the contrib sequence loss and average over the batches\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(\n",
    "            logits,\n",
    "            self.input_obj.targets,\n",
    "            tf.ones([self.batch_size, self.num_steps], dtype=tf.float32),\n",
    "            average_across_timesteps = False,\n",
    "            average_across_batch = True)\n",
    "\n",
    "        # Update the cost\n",
    "        self.cost = tf.reduce_sum(loss)\n",
    "\n",
    "        # get the prediction accuracy\n",
    "        self.softmax_out = tf.nn.softmax(tf.reshape(logits, [-1, vocab_size]))\n",
    "        self.predict = tf.cast(tf.argmax(self.softmax_out, axis=1), tf.int32)\n",
    "        correct_prediction = tf.equal(self.predict, tf.reshape(self.input_obj.targets, [-1]))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        if not is_training:\n",
    "           return\n",
    "        self.learning_rate = tf.Variable(0.0, trainable=False)\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars), 5)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "        # optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train_op = optimizer.apply_gradients(\n",
    "            zip(grads, tvars),\n",
    "            global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "        # self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.cost)\n",
    "\n",
    "        self.new_lr = tf.placeholder(tf.float32, shape=[])\n",
    "        self.lr_update = tf.assign(self.learning_rate, self.new_lr)\n",
    "        \n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(self.lr_update, feed_dict={self.new_lr: lr_value})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainning LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1020 22:38:11.208746 13476 deprecation.py:323] From <ipython-input-11-aec0b10157ce>:11: range_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "W1020 22:38:11.208746 13476 deprecation.py:323] From C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:320: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "W1020 22:38:11.208746 13476 deprecation.py:323] From C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "W1020 22:38:11.224367 13476 deprecation.py:323] From C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "W1020 22:38:11.224367 13476 deprecation.py:323] From C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "W1020 22:38:11.271239 13476 deprecation.py:506] From <ipython-input-13-61330e67be38>:18: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1020 22:38:14.802583 13476 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W1020 22:38:14.833827 13476 deprecation.py:323] From <ipython-input-13-61330e67be38>:30: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W1020 22:38:14.833827 13476 deprecation.py:323] From <ipython-input-13-61330e67be38>:35: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "W1020 22:38:14.833827 13476 rnn_cell_impl.py:1642] At least two cells provided to MultiRNNCell are the same object and will share weights.\n",
      "W1020 22:38:14.833827 13476 deprecation.py:323] From <ipython-input-13-61330e67be38>:37: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W1020 22:38:15.115041 13476 deprecation.py:506] From C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1020 22:38:15.130631 13476 deprecation.py:506] From C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1020 22:38:16.203320 13476 deprecation.py:323] From C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1020 22:38:16.203320 13476 deprecation.py:323] From <ipython-input-13-61330e67be38>:74: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_or_create_global_step\n",
      "W1020 22:38:16.390727 13476 deprecation.py:323] From <ipython-input-14-d8f21a70cf08>:13: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 0, cost: 322.346, accuracy: 0.000, Seconds per step: 0.000\n",
      "Epoch 0, Step 50, cost: 247.691, accuracy: 0.077, Seconds per step: 0.900\n",
      "Epoch 0, Step 100, cost: 232.159, accuracy: 0.067, Seconds per step: 0.900\n",
      "Epoch 1, Step 0, cost: 218.557, accuracy: 0.101, Seconds per step: 0.000\n",
      "Epoch 1, Step 50, cost: 225.501, accuracy: 0.086, Seconds per step: 0.900\n",
      "Epoch 1, Step 100, cost: 221.384, accuracy: 0.109, Seconds per step: 0.900\n",
      "Epoch 2, Step 0, cost: 209.334, accuracy: 0.110, Seconds per step: 0.000\n",
      "Epoch 2, Step 50, cost: 218.158, accuracy: 0.086, Seconds per step: 0.880\n",
      "Epoch 2, Step 100, cost: 215.280, accuracy: 0.104, Seconds per step: 0.780\n",
      "Epoch 3, Step 0, cost: 203.081, accuracy: 0.124, Seconds per step: 0.000\n",
      "Epoch 3, Step 50, cost: 213.844, accuracy: 0.103, Seconds per step: 0.860\n",
      "Epoch 3, Step 100, cost: 209.897, accuracy: 0.120, Seconds per step: 0.840\n",
      "Epoch 4, Step 0, cost: 197.990, accuracy: 0.137, Seconds per step: 0.000\n",
      "Epoch 4, Step 50, cost: 206.438, accuracy: 0.126, Seconds per step: 0.820\n",
      "Epoch 4, Step 100, cost: 208.435, accuracy: 0.130, Seconds per step: 0.780\n",
      "Epoch 5, Step 0, cost: 191.546, accuracy: 0.184, Seconds per step: 0.000\n",
      "Epoch 5, Step 50, cost: 198.116, accuracy: 0.144, Seconds per step: 0.860\n",
      "Epoch 5, Step 100, cost: 202.527, accuracy: 0.143, Seconds per step: 0.820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1020 22:48:06.228312 13476 deprecation.py:323] From C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Step 0, cost: 183.495, accuracy: 0.197, Seconds per step: 0.000\n",
      "Epoch 6, Step 50, cost: 193.490, accuracy: 0.164, Seconds per step: 0.820\n",
      "Epoch 6, Step 100, cost: 200.197, accuracy: 0.149, Seconds per step: 0.780\n",
      "Epoch 7, Step 0, cost: 182.846, accuracy: 0.166, Seconds per step: 0.000\n",
      "Epoch 7, Step 50, cost: 190.050, accuracy: 0.161, Seconds per step: 0.800\n",
      "Epoch 7, Step 100, cost: 195.189, accuracy: 0.156, Seconds per step: 0.840\n",
      "Epoch 8, Step 0, cost: 180.570, accuracy: 0.189, Seconds per step: 0.000\n",
      "Epoch 8, Step 50, cost: 185.783, accuracy: 0.160, Seconds per step: 0.860\n",
      "Epoch 8, Step 100, cost: 191.009, accuracy: 0.170, Seconds per step: 0.820\n",
      "Epoch 9, Step 0, cost: 173.134, accuracy: 0.213, Seconds per step: 0.000\n",
      "Epoch 9, Step 50, cost: 182.976, accuracy: 0.156, Seconds per step: 0.760\n",
      "Epoch 9, Step 100, cost: 186.692, accuracy: 0.167, Seconds per step: 0.780\n",
      "Epoch 10, Step 0, cost: 170.524, accuracy: 0.214, Seconds per step: 0.000\n",
      "Epoch 10, Step 50, cost: 179.188, accuracy: 0.163, Seconds per step: 0.820\n",
      "Epoch 10, Step 100, cost: 181.391, accuracy: 0.179, Seconds per step: 0.860\n",
      "Epoch 11, Step 0, cost: 168.745, accuracy: 0.204, Seconds per step: 0.000\n",
      "Epoch 11, Step 50, cost: 176.329, accuracy: 0.161, Seconds per step: 0.840\n",
      "Epoch 11, Step 100, cost: 178.780, accuracy: 0.180, Seconds per step: 0.860\n",
      "Epoch 12, Step 0, cost: 164.255, accuracy: 0.211, Seconds per step: 0.000\n",
      "Epoch 12, Step 50, cost: 169.968, accuracy: 0.186, Seconds per step: 0.920\n",
      "Epoch 12, Step 100, cost: 174.977, accuracy: 0.191, Seconds per step: 0.800\n",
      "Epoch 13, Step 0, cost: 159.870, accuracy: 0.223, Seconds per step: 0.000\n",
      "Epoch 13, Step 50, cost: 167.582, accuracy: 0.191, Seconds per step: 0.780\n",
      "Epoch 13, Step 100, cost: 169.844, accuracy: 0.206, Seconds per step: 0.800\n",
      "Epoch 14, Step 0, cost: 154.699, accuracy: 0.234, Seconds per step: 0.000\n",
      "Epoch 14, Step 50, cost: 163.761, accuracy: 0.191, Seconds per step: 0.820\n",
      "Epoch 14, Step 100, cost: 165.656, accuracy: 0.197, Seconds per step: 0.800\n",
      "Epoch 15, Step 0, cost: 151.805, accuracy: 0.253, Seconds per step: 0.000\n",
      "Epoch 15, Step 50, cost: 160.859, accuracy: 0.210, Seconds per step: 0.780\n"
     ]
    }
   ],
   "source": [
    "def train(train_data, vocabulary, num_layers, num_epochs, batch_size, model_path, model_save_name,\n",
    "          learning_rate=1.0, max_lr_epoch=10, lr_decay=0.93, print_iter=50):\n",
    "    # setup data and models\n",
    "    training_input = Input(batch_size=batch_size, num_steps=35, data=train_data)\n",
    "    m = Model(training_input, is_training=True, hidden_size=650, vocab_size=vocabulary,\n",
    "              num_layers=num_layers)\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    orig_decay = lr_decay\n",
    "    with tf.Session() as sess:\n",
    "        # start threads\n",
    "        sess.run([init_op])\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        saver = tf.train.Saver()\n",
    "        for epoch in range(num_epochs):\n",
    "            new_lr_decay = orig_decay ** max(epoch + 1 - max_lr_epoch, 0.0)\n",
    "#             m.lr_update(m.learning_rate, new_lr_decay)\n",
    "            m.assign_lr(sess, learning_rate * new_lr_decay)\n",
    "            # m.assign_lr(sess, learning_rate)\n",
    "            # print(m.learning_rate.eval(), new_lr_decay)\n",
    "            current_state = np.zeros((num_layers, 2, batch_size, m.hidden_size))\n",
    "            curr_time = dt.datetime.now()\n",
    "            for step in range(training_input.epoch_size):\n",
    "                # cost, _ = sess.run([m.cost, m.optimizer])\n",
    "                if step % print_iter != 0:\n",
    "                    cost, _, current_state = sess.run([m.cost, m.train_op, m.state],\n",
    "                                                      feed_dict={m.init_state: current_state})\n",
    "                else:\n",
    "                    seconds = (float((dt.datetime.now() - curr_time).seconds) / print_iter)\n",
    "                    curr_time = dt.datetime.now()\n",
    "                    cost, _, current_state, acc = sess.run([m.cost, m.train_op, m.state, m.accuracy],\n",
    "                                                           feed_dict={m.init_state: current_state})\n",
    "                    print(\"Epoch {}, Step {}, cost: {:.3f}, accuracy: {:.3f}, Seconds per step: {:.3f}\".format(epoch,\n",
    "                            step, cost, acc, seconds))\n",
    "\n",
    "            # save a model checkpoint\n",
    "            saver.save(sess, model_path + '\\\\' + model_save_name, global_step=epoch)\n",
    "        # do a final save\n",
    "        saver.save(sess, model_path + '\\\\' + model_save_name + '-final')\n",
    "        # close threads\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        \n",
    "# train(num_corpus, final_embeddings, vocabulary = 10000, 2, num_epochs = 60, batch_size = 20, model_save_name = 'two-layer-lstm-medium-config-60-epoch-0p93-lr-decay-10-max-lr', learning_rate=1.0, max_lr_epoch=10, lr_decay=0.93, print_iter=50)\n",
    "train(train_data, 10000, 2, 15, 20, 'models','two-layer-lstm-medium-config-60-epoch-0p93-lr-decay-10-max-lr', 1.0, 10, 0.93, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in converted code:\n    relative to C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python:\n\n    ops\\rnn_cell_impl.py:1719 call *\n        cur_inp, new_state = cell(cur_inp, cur_state)\n    ops\\rnn_cell_impl.py:1159 __call__\n        inputs, state, cell_call_fn=self.cell.__call__, scope=scope)\n    ops\\rnn_cell_impl.py:1436 _call_wrapped_cell\n        output, new_state = cell_call_fn(inputs, state, **kwargs)\n    ops\\rnn_cell_impl.py:385 __call__\n        self, inputs, state, scope=scope, *args, **kwargs)\n    layers\\base.py:537 __call__\n        outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n    keras\\engine\\base_layer.py:591 __call__\n        self._maybe_build(inputs)\n    keras\\engine\\base_layer.py:1881 _maybe_build\n        self.build(input_shapes)\n    keras\\utils\\tf_utils.py:295 wrapper\n        output_shape = fn(instance, input_shape)\n    ops\\rnn_cell_impl.py:957 build\n        partitioner=maybe_partitioner)\n    keras\\engine\\base_layer.py:1484 add_variable\n        return self.add_weight(*args, **kwargs)\n    layers\\base.py:450 add_weight\n        **kwargs)\n    keras\\engine\\base_layer.py:384 add_weight\n        aggregation=aggregation)\n    training\\tracking\\base.py:663 _add_variable_with_custom_getter\n        **kwargs_for_getter)\n    ops\\variable_scope.py:1496 get_variable\n        aggregation=aggregation)\n    ops\\variable_scope.py:1239 get_variable\n        aggregation=aggregation)\n    ops\\variable_scope.py:545 get_variable\n        return custom_getter(**custom_getter_kwargs)\n    ops\\rnn_cell_impl.py:251 _rnn_get_variable\n        variable = getter(*args, **kwargs)\n    ops\\variable_scope.py:514 _true_getter\n        aggregation=aggregation)\n    ops\\variable_scope.py:864 _get_single_variable\n        (err_msg, \"\".join(traceback.format_list(tb))))\n\n    ValueError: Variable rnn/multi_rnn_cell/cell_0/lstm_cell/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n    \n      File \"C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\n        self._traceback = tf_stack.extract_stack()\n      File \"C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3616, in create_op\n        op_def=op_def)\n      File \"C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n        return func(*args, **kwargs)\n      File \"C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n        op_def=op_def)\n      File \"C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 2023, in variable_v2\n        shared_name=shared_name, name=name)\n    \n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-e471be3b3343>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;31m# train(num_corpus, final_embeddings, vocabulary = 10000, 2, num_epochs = 60, batch_size = 20, model_save_name = 'two-layer-lstm-medium-config-60-epoch-0p93-lr-decay-10-max-lr', learning_rate=1.0, max_lr_epoch=10, lr_decay=0.93, print_iter=50)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_corpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'models'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'two-layer-lstm-medium-config-60-epoch-0p93-lr-decay-10-max-lr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.93\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-e471be3b3343>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_data, embeddings, vocabulary, num_layers, num_epochs, batch_size, model_path, model_save_name, learning_rate, max_lr_epoch, lr_decay, print_iter)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtraining_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m35\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     m = Model(training_input, embeddings, is_training=True, hidden_size=650, vocab_size=vocabulary,\n\u001b[1;32m----> 6\u001b[1;33m               num_layers=num_layers)\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0minit_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0morig_decay\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr_decay\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-039788887821>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input, embeddings, is_training, hidden_size, vocab_size, num_layers, dropout, init_scale)\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mcell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultiRNNCell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcell\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_is_tuple\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrnn_tuple_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[1;31m# reshape to (batch_size * num_steps, hidden_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m               instructions)\n\u001b[1;32m--> 324\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[1;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[0;32m    705\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m         \u001b[0msequence_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 707\u001b[1;33m         dtype=dtype)\n\u001b[0m\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m     \u001b[1;31m# Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m_dynamic_rnn_loop\u001b[1;34m(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[0mparallel_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[0mmaximum_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 916\u001b[1;33m       swap_memory=swap_memory)\n\u001b[0m\u001b[0;32m    917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m   \u001b[1;31m# Unpack final output if not using output tuples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[0;32m   3499\u001b[0m       \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3500\u001b[0m     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants,\n\u001b[1;32m-> 3501\u001b[1;33m                                     return_same_structure)\n\u001b[0m\u001b[0;32m   3502\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmaximum_iterations\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3503\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mBuildLoop\u001b[1;34m(self, pred, body, loop_vars, shape_invariants, return_same_structure)\u001b[0m\n\u001b[0;32m   3010\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3011\u001b[0m         original_body_result, exit_vars = self._BuildLoop(\n\u001b[1;32m-> 3012\u001b[1;33m             pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[0;32m   3013\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3014\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36m_BuildLoop\u001b[1;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[0;32m   2935\u001b[0m         expand_composites=True)\n\u001b[0;32m   2936\u001b[0m     \u001b[0mpre_summaries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2937\u001b[1;33m     \u001b[0mbody_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2938\u001b[0m     \u001b[0mpost_summaries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2939\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_sequence_or_composite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(i, lv)\u001b[0m\n\u001b[0;32m   3454\u001b[0m         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\n\u001b[0;32m   3455\u001b[0m             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\n\u001b[1;32m-> 3456\u001b[1;33m         \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3457\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m_time_step\u001b[1;34m(time, output_ta_t, state)\u001b[0m\n\u001b[0;32m    882\u001b[0m           skip_conditionals=True)\n\u001b[0;32m    883\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m       \u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m     \u001b[1;31m# Keras cells always wrap state as list, even if it's a single tensor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    868\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_keras_rnn_cell\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m       \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 870\u001b[1;33m     \u001b[0mcall_cell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, state, scope)\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope_attrname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mscope\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNNCell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_rnn_get_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgetter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m       \u001b[1;31m# Actually call layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    632\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m                   \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    147\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in converted code:\n    relative to C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python:\n\n    ops\\rnn_cell_impl.py:1719 call *\n        cur_inp, new_state = cell(cur_inp, cur_state)\n    ops\\rnn_cell_impl.py:1159 __call__\n        inputs, state, cell_call_fn=self.cell.__call__, scope=scope)\n    ops\\rnn_cell_impl.py:1436 _call_wrapped_cell\n        output, new_state = cell_call_fn(inputs, state, **kwargs)\n    ops\\rnn_cell_impl.py:385 __call__\n        self, inputs, state, scope=scope, *args, **kwargs)\n    layers\\base.py:537 __call__\n        outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n    keras\\engine\\base_layer.py:591 __call__\n        self._maybe_build(inputs)\n    keras\\engine\\base_layer.py:1881 _maybe_build\n        self.build(input_shapes)\n    keras\\utils\\tf_utils.py:295 wrapper\n        output_shape = fn(instance, input_shape)\n    ops\\rnn_cell_impl.py:957 build\n        partitioner=maybe_partitioner)\n    keras\\engine\\base_layer.py:1484 add_variable\n        return self.add_weight(*args, **kwargs)\n    layers\\base.py:450 add_weight\n        **kwargs)\n    keras\\engine\\base_layer.py:384 add_weight\n        aggregation=aggregation)\n    training\\tracking\\base.py:663 _add_variable_with_custom_getter\n        **kwargs_for_getter)\n    ops\\variable_scope.py:1496 get_variable\n        aggregation=aggregation)\n    ops\\variable_scope.py:1239 get_variable\n        aggregation=aggregation)\n    ops\\variable_scope.py:545 get_variable\n        return custom_getter(**custom_getter_kwargs)\n    ops\\rnn_cell_impl.py:251 _rnn_get_variable\n        variable = getter(*args, **kwargs)\n    ops\\variable_scope.py:514 _true_getter\n        aggregation=aggregation)\n    ops\\variable_scope.py:864 _get_single_variable\n        (err_msg, \"\".join(traceback.format_list(tb))))\n\n    ValueError: Variable rnn/multi_rnn_cell/cell_0/lstm_cell/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n    \n      File \"C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\n        self._traceback = tf_stack.extract_stack()\n      File \"C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3616, in create_op\n        op_def=op_def)\n      File \"C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n        return func(*args, **kwargs)\n      File \"C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n        op_def=op_def)\n      File \"C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 2023, in variable_v2\n        shared_name=shared_name, name=name)\n    \n"
     ]
    }
   ],
   "source": [
    "def train_pre_embedding(train_data, embeddings, vocabulary, num_layers, num_epochs, batch_size, model_path, model_save_name,\n",
    "          learning_rate=1.0, max_lr_epoch=10, lr_decay=0.93, print_iter=50):\n",
    "    # setup data and models\n",
    "    training_input = Input(batch_size=batch_size, num_steps=35, data=train_data)\n",
    "    m = Model_pre_embedding(training_input, embeddings, is_training=True, hidden_size=650, vocab_size=vocabulary,\n",
    "              num_layers=num_layers)\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    orig_decay = lr_decay\n",
    "    with tf.Session() as sess:\n",
    "        # start threads\n",
    "        sess.run([init_op])\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        saver = tf.train.Saver()\n",
    "        for epoch in range(num_epochs):\n",
    "            new_lr_decay = orig_decay ** max(epoch + 1 - max_lr_epoch, 0.0)\n",
    "#             m.lr_update(m.learning_rate, new_lr_decay)\n",
    "            m.assign_lr(sess, learning_rate * new_lr_decay)\n",
    "            # m.assign_lr(sess, learning_rate)\n",
    "            # print(m.learning_rate.eval(), new_lr_decay)\n",
    "            current_state = np.zeros((num_layers, 2, batch_size, m.hidden_size))\n",
    "            curr_time = dt.datetime.now()\n",
    "            for step in range(training_input.epoch_size):\n",
    "                # cost, _ = sess.run([m.cost, m.optimizer])\n",
    "                if step % print_iter != 0:\n",
    "                    cost, _, current_state = sess.run([m.cost, m.train_op, m.state],\n",
    "                                                      feed_dict={m.init_state: current_state})\n",
    "                else:\n",
    "                    seconds = (float((dt.datetime.now() - curr_time).seconds) / print_iter)\n",
    "                    curr_time = dt.datetime.now()\n",
    "                    cost, _, current_state, acc = sess.run([m.cost, m.train_op, m.state, m.accuracy],\n",
    "                                                           feed_dict={m.init_state: current_state})\n",
    "                    print(\"Epoch {}, Step {}, cost: {:.3f}, accuracy: {:.3f}, Seconds per step: {:.3f}\".format(epoch,\n",
    "                            step, cost, acc, seconds))\n",
    "\n",
    "            # save a model checkpoint\n",
    "            saver.save(sess, model_path + '\\\\' + model_save_name, global_step=epoch)\n",
    "        # do a final save\n",
    "        saver.save(sess, model_path + '\\\\' + model_save_name + '-final')\n",
    "        # close threads\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        \n",
    "# train(num_corpus, final_embeddings, vocabulary = 10000, 2, num_epochs = 60, batch_size = 20, model_save_name = 'two-layer-lstm-medium-config-60-epoch-0p93-lr-decay-10-max-lr', learning_rate=1.0, max_lr_epoch=10, lr_decay=0.93, print_iter=50)\n",
    "train(num_corpus, final_embeddings, 10000, 2, 60, 20, 'models','two-layer-lstm-medium-config-60-epoch-0p93-lr-decay-10-max-lr', 1.0, 10, 0.93, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(model_path, test_data, reversed_dictionary):\n",
    "    test_input = Input(batch_size=20, num_steps=35, data=test_data)\n",
    "    m = Model(test_input, is_training=False, hidden_size=650, vocab_size=vocabulary,\n",
    "              num_layers=2)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        # start threads\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        current_state = np.zeros((2, 2, m.batch_size, m.hidden_size))\n",
    "        # restore the trained model\n",
    "        saver.restore(sess, model_path)\n",
    "        # get an average accuracy over num_acc_batches\n",
    "        num_acc_batches = 30\n",
    "        check_batch_idx = 25\n",
    "        acc_check_thresh = 5\n",
    "        accuracy = 0\n",
    "        for batch in range(num_acc_batches):\n",
    "            if batch == check_batch_idx:\n",
    "                true_vals, pred, current_state, acc = sess.run([m.input_obj.targets, m.predict, m.state, m.accuracy],\n",
    "                                                               feed_dict={m.init_state: current_state})\n",
    "                pred_string = [reversed_dictionary[x] for x in pred[:m.num_steps]]\n",
    "                true_vals_string = [reversed_dictionary[x] for x in true_vals[0]]\n",
    "                print(\"True values (1st line) vs predicted values (2nd line):\")\n",
    "                print(\" \".join(true_vals_string))\n",
    "                print(\" \".join(pred_string))\n",
    "            else:\n",
    "                acc, current_state = sess.run([m.accuracy, m.state], feed_dict={m.init_state: current_state})\n",
    "            if batch >= acc_check_thresh:\n",
    "                accuracy += acc\n",
    "        print(\"Average accuracy: {:.3f}\".format(accuracy / (num_acc_batches-acc_check_thresh)))\n",
    "        # close threads\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "\n",
    "\n",
    "if args.data_path:\n",
    "    data_path = args.data_path\n",
    "train_data, valid_data, test_data, vocabulary, reversed_dictionary = load_data()\n",
    "if args.run_opt == 1:\n",
    "    train(train_data, vocabulary, num_layers=2, num_epochs=60, batch_size=20,\n",
    "          model_save_name='two-layer-lstm-medium-config-60-epoch-0p93-lr-decay-10-max-lr')\n",
    "else:\n",
    "    trained_model = args.data_path + \"\\\\two-layer-lstm-medium-config-60-epoch-0p93-lr-decay-10-max-lr-38\"\n",
    "    test(trained_model, test_data, reversed_dictionary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
