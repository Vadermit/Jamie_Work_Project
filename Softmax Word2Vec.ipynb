{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Word2Vec method\n",
    "\n",
    "> About the autor: Jinming Yang is currently an undergraduate school student at Sun Yet-san University who focuses on transportation research and machine learning.\n",
    "\n",
    "For a 10000 word vocabulary, using one-hot vectors as the representation of each word in a neural network is computational expensive. Besides, one-hot vectors can not represent the correlations between words like \"Soviet\" and \"Union\", \"United\" and \"States\"... So we need a new representation of those words. These new representations of words have much lower dimension like 128. The correlations between strong connected words can be reflected in their corresponding representations.\n",
    "\n",
    "In order to do that, **Word2Vec** was introduced to derive those word representations. Hereby, we'll mainly focuse on the **Softmax Word2Vec method**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "Basically the Softmax Word2Vec method use an three layer autoencoder neural network to derive word representations. The first layer is the input layer consists of 10000 neurons(units). The second layer is the hidden layer which consists of 128 neurons(no activation function). The third layer is the softmax layer which consists of 10000 neurons.\n",
    "\n",
    "**Network setting**\n",
    "\n",
    "|`Network Layer`  |`Number of neurons`|`Activation`|\n",
    "|:----------------|------------------:|-----------:|\n",
    "|**Input layer**  |     **10000**     |  **None**  |\n",
    "|**Hidden layer** |      **128**      |  **None**  |\n",
    "|**Output layer** |     **10000**     |**Sigmoid** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the corpus\n",
    "Downloading the corpus in \"mattmahoney.net/dc/text8.zip\". For researchers in main land China the website won't be available. You can also download it in my GitHub repositry \"Learning Machine Learning\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter \n",
      " pierre <unk> N years old will join the board as a nonexecutive director nov. N \n",
      " mr. <unk> is chairman of <unk> n.v. the dutch publishing group \n",
      " rudolph <unk> N years old and former chairman of consolidated gold fields plc was named a nonexecutive director of this british industrial conglomerate \n",
      " a form of asbestos once used to make kent cigarette filters has caused a high percentage of cancer deaths among a group of workers exposed to it more than N years ago researchers reported \n",
      " the asbestos fiber <unk> is unusually <unk> once it enters the <unk> with even brief exposures to it causing symptoms that show up decades later researchers said \n",
      " <unk> inc. the unit of new york-based <unk> corp. that makes kent cigarettes stopped using <unk> in its <unk> cigarette filters in N \n",
      " although preliminary findings wer\n"
     ]
    }
   ],
   "source": [
    "#Read the file\n",
    "f = open(\"ptbdata/ptb.train.txt\",\"r\")\n",
    "rawData = f.read()\n",
    "print(rawData[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After opening the file, we can transfer the content in the file to string format. Then we can split the text string to individual words by blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split:\n",
      "['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack-food', 'ssangyong', 'swapo', 'wachter', 'pierre', '<unk>', 'N', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', 'N', 'mr.', '<unk>', 'is', 'chairman', 'of', '<unk>', 'n.v.', 'the', 'dutch', 'publishing', 'group', 'rudolph', '<unk>', 'N', 'years', 'old', 'and', 'former', 'chairman', 'of', 'consolidated', 'gold', 'fields', 'plc', 'was', 'named', 'a', 'nonexecutive', 'director', 'of', 'this', 'british', 'industrial', 'conglomerate', 'a', 'form', 'of', 'asbestos', 'once', 'used', 'to', 'make', 'kent', 'cigarette', 'filters', 'has', 'caused', 'a', 'high', 'percentage', 'of', 'cancer', 'deaths', 'among', 'a', 'group', 'of', 'workers', 'exposed', 'to', 'it']\n",
      "887521\n"
     ]
    }
   ],
   "source": [
    "# Transfer the raw data as strings using Tensorflow. \n",
    "# Then split it into individual words\n",
    "dataStr = tf.compat.as_str(rawData) #Convert to string\n",
    "print('Split:')\n",
    "data = dataStr.split() #Split by blank\n",
    "print(data[:100])\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dictionary and numerical representation of the corpus\n",
    "We have the words sequence in the corpus. Now we want to use numbers to represent each word and create the dictionary of words and their corresponding number. After that, we can convert the whole text from word sequence to number sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count word number\n",
    "First, count the number of occurence of each word in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999 different words were found in the corpus\n",
      "\n",
      "Part of the dictionary:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'aer': 1,\n",
       " 'banknote': 1,\n",
       " 'berlitz': 1,\n",
       " 'calloway': 1,\n",
       " 'centrust': 1,\n",
       " 'cluett': 1,\n",
       " 'fromstein': 1,\n",
       " 'gitano': 1,\n",
       " 'guterman': 1,\n",
       " 'hydro-quebec': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counted_words = collections.Counter(data)  #counted_words is a dictionary {'word1': num_1, 'word2': num_2, ...}\n",
    "print('%d different words were found in the corpus'%(len(counted_words)))\n",
    "print()\n",
    "print('Part of the dictionary:')\n",
    "dict(list(counted_words.items())[0:10])    #print 10 of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select top frequent words\n",
    "From above, we can see that there are 253854 diffenent words in the corpus. However most of them have very few occurence, building a dictionary for them is highly uneconomical and inefficient. So we just choose the top 9999 frequent words to build the dictionary. The rest of the words which are less likely to occur were classified as 'LFW' aka 'Low Frequency Words'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9999 words in the selected dictionary\n"
     ]
    }
   ],
   "source": [
    "freq_counted_words = dict(counted_words.most_common(9999))\n",
    "print('There are %d words in the selected dictionary'%(len(freq_counted_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now conbine all other less frequently occured words as 'LFW', count their number of occurence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10000 words in the dictionary\n",
      "\n",
      "The first 5 entries in the dictionary is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lfw': 0, 'the': 50770, '<unk>': 45020, 'N': 32481, 'of': 24400}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfw_count = 0\n",
    "for word in counted_words:\n",
    "    if not (word in freq_counted_words.keys()):\n",
    "        lfw_count += 1\n",
    "word_count_dict = {'lfw': lfw_count}\n",
    "word_count_dict.update(freq_counted_words)\n",
    "print('There are %d words in the dictionary'%(len(word_count_dict)))\n",
    "print()\n",
    "print('The first 5 entries in the dictionary is:')\n",
    "dict(list(word_count_dict.items())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already selected the top 10000 frequent words in the corpus. Now we need to index them with numbers aka establish the word2number projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 entries in the dictionary \"word_dict\" is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lfw': 0, 'the': 1, '<unk>': 2, 'N': 3, 'of': 4}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = 0\n",
    "word_dict = {}\n",
    "for word in word_count_dict:\n",
    "    word_dict.update({word: ind})\n",
    "    ind += 1\n",
    "print('The first 5 entries in the dictionary \"word_dict\" is: ')\n",
    "dict(list(word_dict.items())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have already built the dictionary of words and their corresponding number(index). We can now transfer the initial corpus to a number sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 100 words in the corpus represented by their corresponding indeces are:\n",
      "[9970, 9971, 9972, 9973, 9974, 9975, 9976, 9977, 9978, 9979, 9980, 9981, 9982, 9983, 9984, 9985, 9986, 9987, 9988, 9989, 9990, 9991, 9992, 9993, 8569, 2, 3, 72, 393, 33, 2116, 1, 146, 19, 6, 8570, 275, 407, 3, 23, 2, 13, 141, 4, 2, 5278, 1, 3055, 1581, 96, 7232, 2, 3, 72, 393, 8, 337, 141, 4, 2468, 657, 2158, 949, 24, 521, 6, 8570, 275, 4, 39, 303, 437, 3661, 6, 941, 4, 3143, 495, 262, 5, 137, 5882, 4219, 5883, 30, 986, 6, 240, 755, 4, 1013, 2765, 211, 6, 96, 4, 427, 4060, 5, 14]\n"
     ]
    }
   ],
   "source": [
    "num_corpus = []\n",
    "for word in data:\n",
    "    if word in word_dict.keys():\n",
    "        num_corpus.append(word_dict[word])\n",
    "    else:\n",
    "        num_corpus.append(word_dict['lfw'])\n",
    "print('The first 100 words in the corpus represented by their corresponding indeces are:')\n",
    "print(num_corpus[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the dictionary to project words to indeces. We now have to build a dictionary to project each index to its corresponding word. So that we can recover text from index sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 entries in dictionary index_dict is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'lfw',\n",
       " 1: 'the',\n",
       " 2: '<unk>',\n",
       " 3: 'N',\n",
       " 4: 'of',\n",
       " 5: 'to',\n",
       " 6: 'a',\n",
       " 7: 'in',\n",
       " 8: 'and',\n",
       " 9: \"'s\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_dict = dict(zip(word_dict.values(), word_dict.keys()))\n",
    "print('The first 10 entries in dictionary index_dict is: ')\n",
    "dict(list(index_dict.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-gram\n",
    "\n",
    "The **skip-gram** conscept is used to create batches for the autoencoder neural network. It basically has two main conscepts:\n",
    "\n",
    "* **gram**: **gram** is a fix-sized sliding window over the corpus.\n",
    "\n",
    "* **skip**: **skip** is the number of times a word repeated in the dataset with different context words.\n",
    "\n",
    "The central word of the gram is called the **target**. And the rest of the words in the gram is the context words of the **target**. Utilising every context word may be computational expensive, thus we just randomly choose **skip** different context words for one **target** to train the autoencoder neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network inputs and their labels\n",
    "\n",
    "Based on the conscept above, the input of the neural network is the initial representation of each **target** word. For example, in this experiment setting, the input would be a 10000 dimension one-hot vector denoting each **target**. And the label would also be a 10000 dimension one-hot vector denoting one of the context word of that  **target**. Basically, the inputs and their label are this kind of combination: \n",
    "\n",
    "${[target_1, context_1^{(i)}], [target_1, context_2^{(i)}], ..., [target_1, context_{skip}^{(i)}], ..., [target_{ng}, context_1^{(i)}], [target_{ng}, context_2^{(i)}], [target_{ng}, context_{skip}^{(i)}]}$.\n",
    "\n",
    "<img src=\"images/skip_gram.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "The following function can generate a mini-batch with size **batch_size**. **$target$** and **context** were generated separatley, both of which have the length of **batch_size**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "# generate batch data\n",
    "def generate_batch(data, batch_size, skip, sub_gram):\n",
    "    global data_index\n",
    "    assert batch_size % skip == 0\n",
    "    assert skip <= 2 * sub_gram\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    context = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * sub_gram + 1  # [ sub_gram input_word sub_gram]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // skip):\n",
    "        target = sub_gram  # input word at the center of the buffer\n",
    "        targets_to_avoid = [sub_gram]\n",
    "        for j in range(skip):\n",
    "            while target in targets_to_avoid:\n",
    "                target = np.random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * skip + j] = buffer[sub_gram]  # this is the input word\n",
    "            context[i * skip + j, 0] = buffer[target]  # these are the context words\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder Network Setting\n",
    "Neural network settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 10000\n",
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector aka the number of units in the hidden layer.\n",
    "sub_gram = 2          # (Span_of_gram-1)/2\n",
    "skip = 2              # How many times to reuse an input to generate a context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set placeholders for inputs and outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "train_context = tf.placeholder(tf.int32, shape=[batch_size, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set variables in the network: the embedding here is a $(10000\\times128)$ weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vader\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# weight matrix between input layer and hidden layer\n",
    "embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "\n",
    "# get the corresponding embedding of input word.\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "# weight matrix between hidden layer and the softmax layer\n",
    "weights = tf.Variable(tf.truncated_normal([embedding_size, vocabulary_size],\n",
    "                          stddev=1.0 / math.sqrt(embedding_size)))\n",
    "\n",
    "# biases of softmax layer\n",
    "biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "hidden_out = tf.matmul(embed, weights) + biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the context into a one-hot format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_hot = tf.one_hot(train_context, vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using crossentropy as the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-344527f4b59a>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hidden_out, \n",
    "    labels=train_one_hot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a gradient descent minimizer for the **cross_entropy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vader\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# Construct the SGD optimizer using a learning rate of 1.0.\n",
    "optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize all the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  500 :  7.679187176704406\n",
      "Average loss at step  1000 :  7.100021776199341\n",
      "Average loss at step  1500 :  6.862011333465576\n",
      "Average loss at step  2000 :  6.782251722335816\n",
      "Average loss at step  2500 :  6.570901042938233\n",
      "Average loss at step  3000 :  6.7042559719085695\n",
      "Average loss at step  3500 :  6.549054152488709\n",
      "Average loss at step  4000 :  6.45622038936615\n",
      "Average loss at step  4500 :  6.593372061729431\n",
      "Average loss at step  5000 :  6.632398215293884\n",
      "Average loss at step  5500 :  6.549637966156006\n",
      "Average loss at step  6000 :  6.408308345794678\n",
      "Average loss at step  6500 :  6.5721294832229615\n",
      "Average loss at step  7000 :  6.432288290023804\n",
      "Average loss at step  7500 :  6.54589998626709\n",
      "Average loss at step  8000 :  6.486287582397461\n",
      "Average loss at step  8500 :  6.530690508365631\n",
      "Average loss at step  9000 :  6.442089038848877\n",
      "Average loss at step  9500 :  6.494717345237732\n",
      "Average loss at step  10000 :  6.380370049476624\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10000\n",
    "with tf.Session() as session:\n",
    "  # We must initialize all variables before we use them.\n",
    "  init.run()\n",
    "  print('Initialized')\n",
    "\n",
    "  average_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batch_inputs, batch_context = generate_batch(num_corpus,\n",
    "        batch_size, skip, sub_gram)\n",
    "    feed_dict = {train_inputs: batch_inputs, train_context: batch_context}\n",
    "\n",
    "    # We perform one update step by evaluating the optimizer op (including it\n",
    "    # in the list of returned values for session.run()\n",
    "    _, loss_val = session.run([optimizer, cross_entropy], feed_dict=feed_dict)\n",
    "    average_loss += loss_val\n",
    "\n",
    "    if (step + 1) % 500 == 0:\n",
    "        average_loss /= 500\n",
    "        # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "        print('Average loss at step ', step + 1, ': ', average_loss)\n",
    "        average_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
